from torch.autograd import Variable
import torchvision.utils as tvut

from tensorboardX import SummaryWriter
import shutil
from tqdm import tqdm
import numpy as np
import utils

import os
import pdb
import pickle
import argparse

import warnings
warnings.filterwarnings("ignore")

# Torch imports
import torch
import torch.nn as nn
import torch.optim as optim

# Numpy & Scipy imports
import numpy as np
import scipy
import scipy.misc


# torch.set_default_tensor_type('torch.cuda.FloatTensor')

class Trainer:
    def __init__(self, model, loss, train_loader, test_loader, params):
        self.model = model
        self.params = params
        self.params['start_epoch'] = 0

        self.train_loader = train_loader
        self.test_loader = test_loader

        self.fixed_X = utils.to_var(iter(test_loader).next()[0])
        print(self.fixed_X.size())

        self.loss = loss
        self.optimizer = self.get_optimizer()
        # print()
        self.summary_writer = SummaryWriter(log_dir=self.params['summary_dir'])
        print(next(model.parameters()).is_cuda)

    def train(self, opts):
        self.model.train()
        kwargs = {}
        last_loss = 10000000
        save_new_checkpoint = False
        for epoch in range(self.params['start_epoch'], self.params['num_epochs']):
            loss_list = []
            print("epoch {}...".format(epoch))
            for batch_idx, (data, _) in enumerate(tqdm(self.train_loader)):
                if torch.cuda.is_available():
#                    print('using GPU')
                    data = data.cuda()
                data = Variable(data)
                self.optimizer.zero_grad()
                recon_batch, mu, logvar = self.model.forward1(data)
                loss = self.loss(recon_batch, data, mu, logvar)
                loss.backward()
                self.optimizer.step()
                loss_list.append(loss.data[0].item())

            print("epoch {}: - training loss: {}".format(epoch, np.mean(loss_list)))
            new_lr = self.adjust_learning_rate(epoch)
            print('learning rate:', new_lr)

            if epoch % (opts.test_every//2) == 0:
                new_loss = self.test(epoch)
                if new_loss < last_loss:
                    self.save_checkpoint({
                        'epoch': epoch + 1,
                        'state_dict': self.model.state_dict(),
                        'optimizer': self.optimizer.state_dict(),
                    })
                    print("Saved new checkpoint!")
                last_loss = new_loss

            if epoch % opts.test_every == 0:
                self.summary_writer.add_scalar('training/loss', np.mean(loss_list), epoch)
                self.summary_writer.add_scalar('training/learning_rate', new_lr, epoch)
                # self.save_checkpoint({
                #     'epoch': epoch + 1,
                #     'state_dict': self.model.state_dict(),
                #     'optimizer': self.optimizer.state_dict(),
                # })
                # self.print_image("training/epoch"+str(epoch))
                self.save_samples(epoch)

    def test(self, cur_epoch):
        print('testing...')
        self.model.eval()
        test_loss = 0
        mse_loss = 0
        kld_loss = 0
        for i, (data, _) in enumerate(self.test_loader):
            if torch.cuda.is_available():
                data = data.cuda()
            data = Variable(data)
            recon_batch, mu, logvar = self.model.forward1(data)
            test_loss += self.loss(recon_batch, data, mu, logvar).data[0]
            mse_loss += self.loss.mse(recon_batch, data).data[0]
            kld_loss += self.loss.kld(mu, logvar).data[0]

        test_loss /= len(self.test_loader.dataset)
        mse_loss /= len(self.test_loader.dataset)
        kld_loss /= len(self.test_loader.dataset)
        print('====> Test set loss: {:.4f}'.format(test_loss))
        print('====> Test set MSE loss: {:.4f}'.format(mse_loss))
        print('====> Test set KLD loss: {:.4f}'.format(kld_loss))
        self.summary_writer.add_scalar('testing/loss', test_loss, cur_epoch)
        self.summary_writer.add_scalar('testing/mseloss', mse_loss, cur_epoch)
        self.summary_writer.add_scalar('testing/kldloss', kld_loss, cur_epoch)
        self.model.train()
        return test_loss

    def merge_images(self, sources, targets, k=10):
        """Creates a grid consisting of pairs of columns, where the first column in
        each pair contains images source images and the second column in each pair
        contains images generated by the CycleGAN from the corresponding images in
        the first column.
        """
        _, _, h, w = sources.shape
        # row = int(np.sqrt(10)) # TODO: 10 is the hardcoded batch size
        row = 4
        merged = np.zeros([3, row*h, row*w*2])
        for idx, (s, t) in enumerate(zip(sources, targets)):
            
            i = idx // row
            j = idx % row
            merged[:, i*h:(i+1)*h, (j*2)*h:(j*2+1)*h] = s
            merged[:, i*h:(i+1)*h, (j*2+1)*h:(j*2+2)*h] = t
        return merged.transpose(1, 2, 0)


    def save_samples(self, epoch):
        """Saves samples from both generators X->Y and Y->X.
        """

        mu, logvar = self.model.encode(self.fixed_X)
        fake_X = self.model.decode(self.model.reparmaterize(mu, logvar))

        X, fake_X = utils.to_data(self.fixed_X), utils.to_data(fake_X)

        merged = self.merge_images(X, fake_X)
        path = os.path.join(os.path.join(os.path.dirname(__file__),'samples'), 'sample-{:06d}.png'.format(epoch))
        scipy.misc.imsave(path, merged)
        print('Saved {}'.format(path))

    # def print_image(self, name, rand=False):
        # batch1 = self.data.train_set[0][0].unsqueeze(0).cuda()
        # inp = Variable(batch1).cuda()
        # self.model.eval()
        # mu, logvar = self.model.encode(batch1)
        # normalized_version = self.model.decode(mu)
        # tvut.save_image(normalized_version[0], name+".png")
        # self.model.train() # Set the model back in training mode

    # def print_image(self, name, rand=False):
    #   batch1 = self.data.train_set[0][0].unsqueeze(0)
    #   self.model.eval()
    #   # mu, logvar = self.model.encode(batch1)
    #   # normalized_version = self.model.decode(mu)
    #   # print(normalized_version)
    #   print(self.model.forward1(batch1)[0].size())
    #   # tvut.save_image(self.data.un_norm(batch1[0]), name+".png")
    #   self.model.train() # Set the model back in training mode

    def get_optimizer(self):
        return optim.Adam(self.model.parameters(), lr=self.params['learning_rate'],
                          weight_decay=self.params['weight_decay'])

    def adjust_learning_rate(self, epoch):
        """Sets the learning rate to the initial LR multiplied by 0.98 every epoch"""
        learning_rate = self.params['learning_rate'] * (self.params['learning_rate_decay'] ** epoch)
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = learning_rate
        return learning_rate

    def save_checkpoint(self, state, is_best=False, filename='checkpoint2.pth.tar'):
        '''
        a function to save checkpoint of the training
        :param state: {'epoch': cur_epoch + 1, 'state_dict': self.model.state_dict(),
                            'optimizer': self.optimizer.state_dict()}
        :param is_best: boolean to save the checkpoint aside if it has the best score so far
        :param filename: the name of the saved file
        '''
        torch.save(state, self.params['checkpoint_dir'] + filename)
        # if is_best:
        #     shutil.copyfile(self.args.checkpoint_dir + filename,
        #                     self.args.checkpoint_dir + 'model_best.pth.tar')
